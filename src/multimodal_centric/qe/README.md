# Plan for Quality Evaluation (QE) Tasks

This document outlines the plan for implementing the Quality Evaluation (QE) tasks within the `src/multimodal_centric/qe/` directory. The core objective is to evaluate the quality of multimodal embeddings generated by the `embedding_converter` pipeline.

## 1. Overall Plan

I will structure the implementation into three main Python modules, each corresponding to a specific QE task:

-   `src/multimodal_centric/qe/matching.py`: For Modality Matching tasks.
-   `src/multimodal_centric/qe/retrieval.py`: For Modality Retrieval tasks.
-   `src/multimodal_centric/qe/alignment.py`: For Modality Alignment tasks.

Each module will contain functions to handle both **Traditional** and **MAG-specific** evaluation scenarios. All modules will use the `utils.embedding_manager` to load the necessary pre-computed embeddings.

## 2. Key Methodologies and References

My approach will be guided by established and recent research in multimodal representation learning.

-   **Core Matching Score**: The fundamental matching and retrieval scores will be based on the cosine similarity between embeddings, a principle popularized by **CLIP**. The **CLIP-score** itself is typically a scaled version of this similarity.
    -   Radford, A., et al. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. arXiv. [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
    -   Hessel, J., et al. (2021). *CLIPScore: A Reference-free Evaluation Metric for Image Captioning*. arXiv. [https://arxiv.org/abs/2104.08718](https://arxiv.org/abs/2104.08718)

-   **Retrieval Enhancement**: For MAG-specific retrieval, the core idea is to leverage the graph structure to enrich node representations before retrieval. This will be inspired by works like **UniGraph2**, which use GNNs to capture topological information alongside multimodal features.
    -   He, Y., et al. (2025). *UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs*. arXiv. [https://arxiv.org/abs/2502.00806](https://arxiv.org/abs/2502.00806)

-   **Fine-Grained Alignment**: The alignment task will go beyond simple similarity scores, incorporating ideas from **FG-CLIP** to assess fine-grained alignment between image regions and textual phrases.
    -   Xie, C., et al. (2025). *FG-CLIP: Fine-Grained Visual and Textual Alignment*. arXiv. [https://arxiv.org/abs/2505.05071](https://arxiv.org/abs/2505.05071)


## 3. Task Implementation Details

### 3.1. Modality Matching

**File:** `matching.py`

This task focuses on calculating a matching score for a given image-text pair.

| Approach | Input | Processing | Output |
| :--- | :--- | :--- | :--- |
| **Traditional** | An arbitrary image embedding and a text embedding. | Calculate the CLIP-score (typically `100 * cosine_similarity`) between the two embeddings. | A single matching score (e.g., 85.0), reflecting context-free alignment. |
| **MAG-specific** | A `node_id` and a `dataset_name`. | 1. Use `embedding_manager` to fetch the target node's features and its neighbors' features. <br> 2. Use a GNN layer (like GCN) to create a neighborhood-enhanced image embedding and a neighborhood-enhanced text embedding. <br> 3. Calculate the CLIP-score between these two **enhanced** embeddings. | A single matching score, reflecting context-aware alignment. |

### 3.2. Modality Retrieval

**File:** `retrieval.py`

This task involves using a query (image or text) to retrieve the most relevant items from a candidate pool.

| Approach | Input | Processing | Output |
| :--- | :--- | :--- | :--- |
| **Traditional** | A query embedding (image or text) and a pool of candidate embeddings (images or texts). | 1. Calculate the cosine similarity between the query embedding and all candidate embeddings. <br> 2. Rank candidates based on similarity scores. | A ranked list of candidate IDs. |
| **MAG-specific** | A query `node_id` and a `dataset_name`. The candidate pool is all other nodes in the same graph. | 1. Fetch the query node's multimodal embedding using `embedding_manager`. <br> 2. **(Enhanced)** Use a GNN to aggregate features from the query node's 1-hop neighbors, creating a neighborhood-enhanced embedding. <br> 3. Calculate similarity between the enhanced query embedding and all other node embeddings. | A ranked list of node IDs from the graph. |

### 3.3. Modality Alignment

**File:** `alignment.py`

This task provides a more detailed, fine-grained assessment of how well visual and textual modalities are aligned. It is divided into two distinct stages: offline data preprocessing and online evaluation.

#### Stage 1: Data Preprocessing (One-time Task)
This stage creates a benchmark dataset for evaluation.
1.  **Phrase Extraction**: For each node's text, use an NLP tool (e.g., SpaCy) to extract key noun phrases.
2.  **Visual Grounding**: Use a powerful, external pre-trained model (e.g., Grounding DINO) to find the corresponding bounding box in the image for each extracted phrase.
3.  **Store Benchmark Data**: Save these `(image_path, text, [(phrase, box), ...])` mappings. This data serves as the ground truth for our evaluation.

#### Stage 2: Evaluation (Implemented in `alignment.py`)
This stage evaluates our own model's fine-grained alignment capability.

| Approach | Input | Processing | Output |
| :--- | :--- | :--- | :--- |
| **Traditional** | A benchmark entry: `(image_path, text, [(phrase, box), ...])`. | 1. Get the image feature map from **our own image encoder**. <br> 2. For each `(phrase, box)` pair: <br> &nbsp;&nbsp; a. Get the phrase embedding from **our own text encoder**. <br> &nbsp;&nbsp; b. Use RoIAlign to extract the region embedding from the image feature map using the `box`. <br> &nbsp;&nbsp; c. Calculate cosine similarity between the region and phrase embeddings. | A list of `(phrase, box, alignment_score)` tuples, reflecting baseline alignment quality. |
| **MAG-specific** | A `node_id` and its corresponding benchmark entry `(image_path, text, [(phrase, box), ...])`. | 1. Get the target node's feature map and its **neighbors' feature maps**. <br> 2. Use a GNN layer to aggregate neighbor features, creating an **enhanced feature map** for the target node. <br> 3. For each `(phrase, box)` pair, perform the same RoIAlign and similarity calculation, but on the **enhanced feature map**. | A list of `(phrase, box, alignment_score)` tuples, reflecting context-aware alignment quality. |
