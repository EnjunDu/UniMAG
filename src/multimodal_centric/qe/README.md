# Multimodal Quality Evaluation (QE)

This folder contains the source code for the Multimodal Quality Evaluation (QE) tasks. The primary goal of this module is to assess the quality and alignment of multimodal embeddings generated by the upstream `embedding_converter` pipeline.

## Task Overview

The QE module implements three distinct downstream tasks to evaluate different facets of multimodal representations on graphs:

1.  **Modality Matching**: Calculates a similarity score for a given image-text pair, enhanced by the graph context.
2.  **Modality Retrieval**: Uses a query in one modality (e.g., text) to retrieve relevant items from a pool of candidates in another modality (e.g., image), leveraging the graph structure.
3.  **Modality Alignment**: Performs a fine-grained evaluation by assessing the alignment between textual phrases and specific image regions (bounding boxes). (Enlightened by [FG-CLIP](https://arxiv.org/abs/2505.05071))

## Architecture

The QE module follows a modular "Orchestrator-Trainer-Evaluator" pattern, driven by Hydra configurations.

-   **`run.py` (Orchestrator)**: Serves as the entry point for all QE tasks when called from the main `src/main.py` dispatcher. It parses the Hydra config (`cfg`) and orchestrates the workflow by instantiating the appropriate trainers and evaluators.
-   **`trainers/`**: Contains classes responsible for model training.
    -   `gnn_trainer.py`: Handles the **Stage 1** training of a GNN model using a contrastive loss (InfoNCE). The trained GNN is shared across all QE tasks to provide context-aware node embeddings. It automatically caches and loads trained models to avoid redundant training.
    -   `retrieval_trainer.py`: Manages the **Stage 2** training for the retrieval task, where a specialized `TwoTowerModel` is trained on top of the GNN-enhanced embeddings. (Note: Only for `modality_retrieval` task)
-   **`evaluators/`**: Contains classes that implement the specific evaluation logic for each task, using the trained models provided by the trainers.
-   **`models/`**: Defines task-specific models, such as the `TwoTowerModel` used for retrieval.

## Configuration

The behavior of the QE module is entirely controlled by Hydra configuration files. Instead of monolithic configs, we use **Config Groups** to compose experiments dynamically.

-   **`configs/task/`**: Defines the parameters unique to each QE task (e.g., `modality_matching.yaml`, `modality_retrieval.yaml`).
-   **`configs/model/`**: Defines the parameters for different GNN models (e.g., `gcn.yaml`, `gat.yaml`).
-   **`configs/dataset/`**: Defines the parameters for different datasets (e.g., `grocery.yaml`, `toys.yaml`).
-   **`configs/config.yaml`**: Contains global defaults and common parameters (e.g., `training` and `embedding` settings) shared across all tasks.

## Running QE Tasks

All QE tasks are launched through the main entry point `src/main.py`. You can combine different task, model, and dataset configurations directly from the command line.

### 1. Modality Matching

**Objective**: Evaluate the matching score between GNN-enhanced image and text embeddings for all nodes in a graph.

**Command Example**:
Run `modality_matching` on the `Grocery` dataset using a `GCN` model:
```bash
python src/main.py task=modality_matching model=gcn dataset=grocery
```

### 2. Modality Retrieval

**Objective**: Evaluate text-to-image and image-to-text retrieval performance. This involves a two-stage training process.

**Command Example**:
Run `modality_retrieval` on the `Toys` dataset using a `GAT` model:
```bash
python src/main.py task=modality_retrieval model=gat dataset=toys
```

### 3. Modality Alignment

**Objective**: Evaluate the fine-grained alignment between text phrases and image regions. This task requires a one-time preprocessing step to generate ground-truth data.

**Step 1: Preprocessing (One-time only per dataset)**
Before running the evaluation for the first time, you must generate the preprocessed data file. This script leverages our Hydra configuration, so you only need to provide the dataset name.

> [!NOTE]
> This script uses a **parallel** version to speed up preprocessing. You can specify the number of workers per GPU using the `--workers-per-gpu` parameter. You can use `0` to skip a GPU. For example, if you have 4 GPUs, you can use `0 3 3 1` to skip the first GPU and use the remaining 3 GPUs.
> This script is divided into two stages:
> 
> > 1. Generate ground-truth data and save it as a `jsonl` file. (This stage is time-consuming, but the loaded model is small, so it is recommended to assign more workers per GPU.)
> > 2. Read the `jsonl` file, generate feature pairs, and save them as a `pt` file. (This stage is fast, but the loaded model is large, so it is recommended to assign fewer workers per GPU.)
>
> You can specify which stage to execute using the `--stage` parameter. (By default, both stages are executed.)

**Command Example**:
Generate preprocessing data for the `Grocery` dataset.

```bash
python src/multimodal_centric/qe/scripts/prepare_alignment_data.py --dataset grocery --workers-per-gpu 0 3 3 1
```

If you want to run the two stages separately (for higher efficiency), you can use the following command:

```bash
# more workers for stage 1
python src/multimodal_centric/qe/scripts/prepare_alignment_data.py --dataset grocery --stage 1 --workers-per-gpu 10 10 10 10 && \
python src/multimodal_centric/qe/scripts/prepare_alignment_data.py --dataset grocery --stage 2 --workers-per-gpu 2 2 2 2
```

**Step 2: Running the Evaluation**
Once the preprocessed file exists, you can run the evaluation.
```bash
python src/main.py task=modality_alignment model=gcn dataset=grocery
```

### Overriding Parameters

You can easily override any parameter from the command line. For example, to change the learning rate and number of epochs for a matching task:
```bash
python src/main.py task=modality_matching model=gcn dataset=grocery training.lr=0.005 training.epochs=100
```

Or you can change the files in `configs/` directly.
